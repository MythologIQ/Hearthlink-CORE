# Changelog

All notable changes to GG-CORE (Greatest Good - Contained Offline Restricted Execution) are documented in this file.

## [0.8.1] - 2026-02-20

### E2E Model Inference Verified

This release fixes critical bugs in the GGUF backend and adds verified E2E testing with real models.

#### Fixed

- **GGUF Batch Logits** (`src/engine/gguf/backend.rs`): Fixed `add_seq()` to compute logits only for the last token in the prompt batch, required for sampling
- **Sampler Index** (`src/engine/gguf/backend.rs`): Fixed `sampler.sample()` to use `-1` (last output) instead of sequence position, matching llama-cpp-2 API expectations

#### Added

- **Speculative Decoding for GGUF** (`src/engine/gguf/speculative.rs`): 2-3x CPU speedup via draft-verify loop
  - `GgufDraftModel`: Wrapper implementing `DraftModel` trait
  - `GgufTargetModel`: Wrapper implementing `TargetModel` trait
  - Backend methods: `generate_from_tokens()`, `verify_tokens()`, `eos_token()`
- **E2E Model Test** (`tests/e2e_model_test.rs`): Real model inference tests with Qwen 2.5 0.5B
  - `e2e_load_and_generate`: Batch generation test
  - `e2e_streaming_generation`: Token-by-token streaming test
  - `e2e_chat_messages`: Chat message formatting with system/user roles
  - `e2e_speculative_decoding`: Speculative decoding integration test
  - `e2e_performance_benchmark`: Throughput measurement (tok/s)
- **Test Scripts**: PowerShell build script for VS2022 + LLVM environment setup

#### Verified

- ✅ GGUF model loading (Qwen 2.5 0.5B, 463 MiB, Q4_K)
- ✅ Batch generation (~40 tok/s on CPU release, ~21 tok/s debug)
- ✅ Streaming generation (20 tokens via async channel)
- ✅ Chat messages with role formatting
- ✅ Flash Attention enabled automatically
- ✅ Memory usage: 435 MiB model + 299 MiB compute + 6 MiB KV cache

#### Benchmark Hardware

- CPU: Intel Core i7-7700K (4c/8t @ 4.2 GHz)
- RAM: 32 GB DDR4-2400
- OS: Windows 10 x64
- Build: Release with `lto = "thin"`, `codegen-units = 1`

---

## [0.8.0] - 2026-02-19

### GG-CORE Rebrand & Extension Point Architecture

This release rebrands from "Veritas SPARK" to "GG-CORE" (Greatest Good - Contained Offline Restricted Execution) and introduces the extension point architecture for commercial multi-tenant features.

#### Added

- **Request Shim Interface** (`src/shim/mod.rs`): Extension point for commercial features
  - `RequestInterceptor` trait for rate limiting, priority tagging, tenant context
  - `PassthroughInterceptor` default no-op implementation
  - `InterceptResult` and `InterceptError` types for interception results
- **Open Core Architecture**: Clear separation between OSS runtime and commercial extensions
  - GG-CORE OSS: Apache 2.0 licensed core runtime
  - GG-CORE Nexus: Commercial extension point (separate repo)

#### Changed

- **Complete Rebrand**: All references updated from Veritas SPARK to GG-CORE
  - `veritas-spark` → `gg-core` (crate name, CLI, socket paths)
  - `VERITAS_SPARK_*` → `GG_CORE_*` (environment variables)
  - Updated all documentation, comments, and branding

#### Philosophy

GG-CORE adopts triage principles ("Greatest Good for the Greatest Number"):
- **C.O.R.E.**: Contained, Offline, Restricted, Execution
- Resource-aware, multi-tenant AI that prioritizes system stability
- Extension points for commercial tiered service models

---

## [0.7.0] - 2026-02-19

### Streaming Inference

This release introduces real token-by-token streaming inference via IPC.

#### Added

- **Streaming Inference**: Token-by-token streaming via IPC with `stream: true` parameter
- **Mid-Stream Cancellation**: Cancel active streaming requests with `CancelRequest` message
- **CLI `infer` Command**: New CLI command for direct inference
  - `gg-core infer --model <MODEL> --prompt <PROMPT>` - Single response
  - `gg-core infer --model <MODEL> --prompt <PROMPT> --stream` - Streaming output
- **IpcStreamBridge**: New adapter for sending streaming chunks to IPC clients
- **StreamChunk.text Field**: Optional decoded text field for client display

#### Changed

- **E2E Test Scripts**: Updated to include streaming verification (steps 5-7)

#### Wire Protocol

New streaming protocol (backward compatible):

```json
// Request with stream: true
{ "type": "inference_request", "request_id": 1, "model_id": "...", "prompt": "...", "parameters": { "stream": true } }

// Multiple response chunks
{ "type": "stream_chunk", "request_id": 1, "token": 15496, "text": "Hello", "is_final": false }
{ "type": "stream_chunk", "request_id": 1, "token": 198, "text": "!", "is_final": true }

// Cancel request
{ "type": "cancel_request", "request_id": 1 }
```

#### Internal

- `process_streaming()` in handler.rs for streaming inference coordination
- `run_stream_sync()` for blocking task integration
- Split read/write connection handling in server.rs
- CancellationToken integration for mid-stream abort

---

## [0.6.7] - 2026-02-19

### Production Safety Fixes

This release focuses on production safety and fail-fast behavior for the COREFORGE integration.

#### Fixed

- **Flash Attention Placeholder**: CUDA and Metal implementations now return explicit errors instead of zero vectors when kernel not implemented
- **Tokenizer Stub Behavior**: `encode()` and `decode()` now return `TokenizerError::NotLoaded` instead of silently returning empty results
- **Handler Metrics**: Fixed hardcoded `ModelHandle::new(0)` - now uses proper model lookup for metrics attribution
- **Telemetry Integration**: Handler now calls `telemetry::record_request_success()` and `record_request_failure()` for Prometheus-compatible metrics
- **FFI Streaming**: Updated to use model_id lookup; token-based API now fails fast with deprecation message
- **Benchmark Protocol**: Updated IPC throughput and scheduler benchmarks to use v0.6.5 text-based protocol

#### Added

- `InferenceEngine::get_handle()` method for model_id to ModelHandle resolution
- 8 new tests for InferenceEngine and InferenceParams validation
- Explicit version roadmap comments for unimplemented status --json fields (v0.7.0+)

#### Changed

- Tokenizer tests updated to expect `NotLoaded` errors instead of empty results
- Prompt fixtures updated to use text-based `prompt` field instead of `prompt_tokens`

### Breaking Changes

- FFI streaming with token arrays now returns `InvalidParams` error
- Stub tokenizer operations now fail instead of returning empty values

---

## [0.6.5] - 2026-02-18

### Text-Based IPC Protocol

- Eliminated mock data paths
- Changed IPC protocol from tokenized to text-based prompts
- Added chaos testing infrastructure

---

## [0.6.0] - 2026-02-17

### Functional GGUF Backend

- Functional GGUF inference via llama-cpp-2
- IPC server implementation
- Chaos testing framework

---

Copyright 2024-2026 GG-CORE Contributors
